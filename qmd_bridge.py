# Sacred Essence v3.1 - QMD Integration Bridge v3.0
# ç¥é«“èˆ‡ QMD çš„æ·±åº¦æ•´åˆæ©‹æ¥å™¨ï¼ˆEdge Cases ä¿®è£œç‰ˆï¼‰
# æ¶æ§‹ï¼šç¥é«“å®šç•Œï¼ˆæ¨¹ç‹€è·¯ç”±ï¼‰+ QMD æ·±æ½›ï¼ˆé™ç¸®æª¢ç´¢ï¼‰+ é€ƒç”Ÿè‰™ Fallback

import subprocess
import json
import os
import re
from typing import List, Dict, Optional, Tuple, Set
from pathlib import Path
from dataclasses import dataclass, asdict
from datetime import datetime

@dataclass
class QMDContext:
    """QMD ä¸Šä¸‹æ–‡ç¶å®šè³‡è¨Š"""
    node_id: str
    parent_id: Optional[str]
    topic: str
    layer: str
    state: str

@dataclass
class SearchResult:
    """çµ±ä¸€æœç´¢çµæœæ ¼å¼"""
    node_id: str
    topic: str
    content: str
    score: float
    source: str  # "constrained", "fallback_bm25", "fallback_vector"
    is_chunk: bool  # æ˜¯å¦ç‚º Chunk ç‰‡æ®µ
    full_path: Optional[str] = None  # å®Œæ•´ L2 æª”æ¡ˆè·¯å¾‘

class QMDBridge:
    """
    ç¥é«“ (Sacred Essence) èˆ‡ QMD çš„æ·±åº¦æ•´åˆæ©‹æ¥å™¨ v3.0
    
    ä¿®è£œçš„ Edge Casesï¼š
    1. é€ƒç”Ÿè‰™æ©Ÿåˆ¶ï¼šç¥é«“ä¿¡å¿ƒä¸è¶³æ™‚ï¼ŒFallback åˆ° QMD å…¨å±€æœç´¢
    2. æ•¸æ“šä¸€è‡´æ€§ï¼šå®šæœŸ Audit æ¸…é™¤å­¤å…’è³‡æ–™
    3. æ€§èƒ½å„ªåŒ–ï¼šè¶…æ™‚æ§åˆ¶ + çµæœé™åˆ¶
    4. ä¸Šä¸‹æ–‡å®Œæ•´æ€§ï¼šæ™ºèƒ½åˆ¤æ–·è¼‰å…¥å®Œæ•´ L2 æˆ– Chunk
    """
    
    # é€ƒç”Ÿè‰™é–¾å€¼è¨­å®š
    FALLBACK_CONFIDENCE_THRESHOLD = 0.3  # ç¥é«“ä¿¡å¿ƒåˆ†æ•¸ä½æ–¼æ­¤å€¼è§¸ç™¼ Fallback
    FALLBACK_MAX_RESULTS = 5  # Fallback æœç´¢æœ€å¤§çµæœæ•¸
    QMD_TIMEOUT = 300  # QMD å‘½ä»¤è¶…æ™‚ç§’æ•¸ï¼ˆåŠ é•·ä»¥é¿å…åœ¨ CPU ä¸Šå¤§é‡ embedding æ™‚å¡æ­»ï¼‰
    
    def __init__(self, collection_name: str = "sacred-l2", memory_dir: Optional[str] = None):
        self.collection_name = collection_name
        self.qmd_cmd = "qmd"
        self.memory_dir = memory_dir or self._default_memory_dir()
        
    def _default_memory_dir(self) -> str:
        """é è¨­ç¥é«“è¨˜æ†¶ç›®éŒ„"""
        return str(Path.home() / ".openclaw" / "workspace" / "memory" / "octagram" / "engine" / "memory" / "topics")
        
    def _run_qmd(self, args: List[str], timeout: int = None) -> Tuple[bool, str]:
        """åŸ·è¡Œ QMD å‘½ä»¤ä¸¦è¿”å›çµæœï¼ˆæ”¯æ´è¶…æ™‚ï¼‰"""
        timeout = timeout or self.QMD_TIMEOUT
        
        # å¼·åˆ¶é—œé–‰ GPUï¼Œé˜²æ­¢ node-llama-cpp åœ¨ WSL æ‰¾ä¸åˆ° CUDA æ™‚ç˜‹ç‹‚å˜—è©¦å¾æºç¢¼ç·¨è­¯å°è‡´å¡æ­»
        env = os.environ.copy()
        env["NODE_LLAMA_CPP_GPU"] = "false"
        
        try:
            result = subprocess.run(
                [self.qmd_cmd] + args,
                capture_output=True,
                text=True,
                timeout=timeout,
                env=env
            )
            if result.returncode == 0:
                return True, result.stdout
            else:
                return False, result.stderr
        except subprocess.TimeoutExpired:
            return False, f"QMD command timeout after {timeout}s"
        except Exception as e:
            return False, str(e)
    
    def _extract_node_id_from_content(self, content: str) -> Optional[str]:
        """å¾å…§å®¹å‰ç¶´æå– node_id"""
        match = re.search(r'\[NODE_ID:([^\]]+)\]', content)
        return match.group(1) if match else None
    
    def _extract_metadata_from_content(self, content: str) -> Dict[str, str]:
        """å¾å…§å®¹å‰ç¶´æå–æ‰€æœ‰ metadata"""
        metadata = {}
        patterns = {
            'node_id': r'\[NODE_ID:([^\]]+)\]',
            'topic': r'\[TOPIC:([^\]]+)\]',
            'state': r'\[STATE:([^\]]+)\]',
            'parent_id': r'\[PARENT:([^\]]+)\]'
        }
        for key, pattern in patterns.items():
            match = re.search(pattern, content)
            if match:
                metadata[key] = match.group(1)
        return metadata
    
    def _clean_content(self, content: str) -> str:
        """ç§»é™¤ metadata å‰ç¶´ï¼Œè¿”å›ä¹¾æ·¨å…§å®¹"""
        return re.sub(r'^\[NODE_ID:[^\]]+\](\[TOPIC:[^\]]+\])?(\[STATE:[^\]]+\])?(\[PARENT:[^\]]+\])?\n', '', content)
    
    def _load_full_l2(self, node_id: str, topic: str) -> Optional[str]:
        """
        ä¿®è£œ Edge Case 4ï¼šè¼‰å…¥å®Œæ•´ L2 å…§å®¹ï¼Œé¿å… Chunk æˆªæ–·
        """
        content_path = Path(self.memory_dir) / topic / node_id / "content.md"
        if content_path.exists():
            try:
                with open(content_path, 'r', encoding='utf-8') as f:
                    return f.read()
            except Exception:
                pass
        return None
    
    def collection_exists(self) -> bool:
        """æª¢æŸ¥é›†åˆæ˜¯å¦å·²å­˜åœ¨"""
        success, output = self._run_qmd(["collection", "list"], timeout=5)
        if success:
            return self.collection_name in output
        return False
    
    # ==================== Edge Case 1: é€ƒç”Ÿè‰™æ©Ÿåˆ¶ ====================
    
    def smart_search_with_fallback(
        self,
        query_text: str,
        node_whitelist: Set[str],
        sacred_confidence: float,
        n_results: int = 5,
        load_full_l2: bool = True,
        max_token_budget: int = 2000
    ) -> Tuple[List[SearchResult], Dict]:
        """
        æ™ºèƒ½æœç´¢ï¼ˆå«é€ƒç”Ÿè‰™ Fallbackï¼‰
        
        ä¿®è£œ Edge Case 1ï¼šç•¶ç¥é«“ä¿¡å¿ƒä¸è¶³æ™‚ï¼Œè‡ªå‹• Fallback åˆ° QMD å…¨å±€æœç´¢
        
        Args:
            query_text: æŸ¥è©¢æ–‡å­—
            node_whitelist: ç¥é«“æä¾›çš„ç™½åå–®
            sacred_confidence: ç¥é«“æª¢ç´¢çš„ä¿¡å¿ƒåˆ†æ•¸ (0-1)
            n_results: è¿”å›çµæœæ•¸é‡
            load_full_l2: æ˜¯å¦è¼‰å…¥å®Œæ•´ L2ï¼ˆé¿å… Chunk æˆªæ–·ï¼‰
            max_token_budget: æœ€å¤§ Token é ç®—
            
        Returns:
            (çµæœåˆ—è¡¨, æœç´¢å…ƒæ•¸æ“š)
        """
        results = []
        metadata = {
            "strategy": "constrained",
            "sacred_confidence": sacred_confidence,
            "fallback_triggered": False,
            "total_nodes_searched": len(node_whitelist)
        }
        
        # Step 1: å˜—è©¦é™ç¸®æœç´¢
        if node_whitelist and sacred_confidence >= self.FALLBACK_CONFIDENCE_THRESHOLD:
            constrained_results = self.constrained_search(
                query_text, node_whitelist, n_results=n_results * 2
            )
            results = self._convert_to_search_results(
                constrained_results, source="constrained", load_full_l2=load_full_l2
            )
        
        # Step 2: é€ƒç”Ÿè‰™æ©Ÿåˆ¶ - æ–·å´–å¼éæ¿¾ä¿®å¾© (The Fallback Gap)
        # å¦‚æœç¥é«“å®Œå…¨æ²’æœ‰æ’ˆåˆ°ç™½åå–®ï¼ˆé–€æ²’é–‹ï¼‰ï¼Œæˆ–è€…ä¿¡å¿ƒä¸è¶³ï¼Œå¼·åˆ¶é€²è¡Œå…¨å±€ç›²æœ
        if not node_whitelist or sacred_confidence < self.FALLBACK_CONFIDENCE_THRESHOLD:
            metadata["fallback_triggered"] = True
            metadata["strategy"] = "fallback_hybrid"
            
            print(f"ğŸš¨ è§¸ç™¼é€ƒç”Ÿè‰™æ©Ÿåˆ¶ (ç¥é«“ä¿¡å¿ƒ: {sacred_confidence:.2f}, ç™½åå–®: {len(node_whitelist)})")
            
            # Fallback 1: å…¨å±€ BM25 é—œéµå­—æœç´¢
            fallback_results = self.keyword_search(query_text, n_results=self.FALLBACK_MAX_RESULTS)
            
            # åˆä½µçµæœï¼ˆå»é‡ï¼‰
            existing_node_ids = {r.node_id for r in results}
            for r in fallback_results:
                node_id = self._extract_node_id_from_content(r.get('content', ''))
                if node_id and node_id not in existing_node_ids:
                    results.append(SearchResult(
                        node_id=node_id,
                        topic=self._extract_metadata_from_content(r.get('content', '')).get('topic', 'unknown'),
                        content=self._clean_content(r.get('content', '')),
                        score=r.get('score', 0),
                        source="fallback_bm25",
                        is_chunk=True,
                        full_path=None
                    ))
                    existing_node_ids.add(node_id)
                    
                    if len(results) >= n_results + self.FALLBACK_MAX_RESULTS:
                        break
        
        # Step 3: æ™ºèƒ½è¼‰å…¥å®Œæ•´ L2ï¼ˆä¿®è£œ Edge Case 4ï¼‰
        if load_full_l2:
            results = self._intelligent_load_full_l2(results, max_token_budget)
        
        # é™åˆ¶çµæœæ•¸é‡
        results = results[:n_results]
        metadata["final_result_count"] = len(results)
        
        return results, metadata
    
    def _convert_to_search_results(
        self, 
        raw_results: List[Dict], 
        source: str,
        load_full_l2: bool = True
    ) -> List[SearchResult]:
        """å°‡åŸå§‹çµæœè½‰æ›ç‚ºçµ±ä¸€æ ¼å¼"""
        converted = []
        for r in raw_results:
            content = r.get('content', '')
            metadata = self._extract_metadata_from_content(content)
            clean_content = self._clean_content(content)
            
            converted.append(SearchResult(
                node_id=metadata.get('node_id', 'unknown'),
                topic=metadata.get('topic', 'unknown'),
                content=clean_content,
                score=r.get('score', 0),
                source=source,
                is_chunk=True,  # QMD è¿”å›çš„é€šå¸¸æ˜¯ Chunk
                full_path=None
            ))
        return converted
    
    def _intelligent_load_full_l2(
        self, 
        results: List[SearchResult], 
        max_token_budget: int
    ) -> List[SearchResult]:
        """
        ä¿®è£œ Edge Case 4ï¼šæ™ºèƒ½åˆ¤æ–·æ˜¯å¦è¼‰å…¥å®Œæ•´ L2
        
        ç­–ç•¥ï¼š
        - å¦‚æœ Chunk é•·åº¦ < 500 tokensï¼Œå˜—è©¦è¼‰å…¥å®Œæ•´ L2
        - å¦‚æœè¼‰å…¥å¾Œç¸½ Token ä¸è¶…éé ç®—ï¼Œå‰‡ä½¿ç”¨å®Œæ•´ L2
        - å¦å‰‡ä¿ç•™ Chunk ä¸¦æ·»åŠ æ¨™è¨˜
        """
        total_tokens = 0
        for r in results:
            # ç°¡å–®ä¼°ç®— token æ•¸ï¼ˆä¸­æ–‡ç´„ 1.5 å­—/ tokenï¼‰
            chunk_tokens = len(r.content) / 1.5
            
            if r.is_chunk and chunk_tokens < 500:
                full_content = self._load_full_l2(r.node_id, r.topic)
                if full_content:
                    full_tokens = len(full_content) / 1.5
                    if total_tokens + full_tokens <= max_token_budget:
                        r.content = full_content
                        r.is_chunk = False
                        total_tokens += full_tokens
                        continue
            
            total_tokens += chunk_tokens
        
        return results
    
    # ==================== Edge Case 2: æ•¸æ“šä¸€è‡´æ€§å¯©è¨ˆ ====================
    
    def audit_and_cleanup(self, dry_run: bool = True) -> Dict:
        """
        ä¿®è£œ Edge Case 2ï¼šæ•¸æ“šä¸€è‡´æ€§å¯©è¨ˆï¼Œæ¸…é™¤å­¤å…’è³‡æ–™
        
        æ¯”å°ç¥é«“ç¯€é»æ¸…å–®èˆ‡ QMD ç´¢å¼•ï¼Œæ‰¾å‡ºï¼š
        - å­¤å…’ QMD è³‡æ–™ï¼ˆQMD æœ‰ä½†ç¥é«“å·²åˆªé™¤ï¼‰
        - ç¼ºå¤±çš„åŒæ­¥ï¼ˆç¥é«“æœ‰ä½† QMD æ²’æœ‰ï¼‰
        
        Args:
            dry_run: å¦‚æœ Trueï¼Œåªå ±å‘Šä¸åˆªé™¤
            
        Returns:
            å¯©è¨ˆå ±å‘Š
        """
        report = {
            "timestamp": datetime.now().isoformat(),
            "orphaned_in_qmd": [],
            "missing_in_qmd": [],
            "synced_correctly": [],
            "actions_taken": []
        }
        
        # 1. æ”¶é›†ç¥é«“ç¯€é»æ¸…å–®
        sacred_nodes = set()
        if os.path.exists(self.memory_dir):
            topics_dir = Path(self.memory_dir)
            for meta_file in topics_dir.rglob("node.meta.json"):
                try:
                    with open(meta_file, 'r', encoding='utf-8') as f:
                        meta = json.load(f)
                    node_id = meta.get('id')
                    state = meta.get('state', 'SILVER')
                    if node_id and state != 'DUST':  # DUST ç¯€é»è¦–ç‚ºå·²åˆªé™¤
                        sacred_nodes.add(node_id)
                except:
                    pass
        
        # 2. æ”¶é›† QMD ç¯€é»æ¸…å–®
        qmd_nodes = set()
        if self.collection_exists():
            # ä½¿ç”¨ qmd ls ç²å–æ‰€æœ‰æ–‡æª”
            success, output = self._run_qmd(["ls", self.collection_name])
            if success:
                for line in output.split('\n'):
                    node_id = self._extract_node_id_from_content(line)
                    if node_id:
                        qmd_nodes.add(node_id)
        
        # 3. æ¯”å°
        report["orphaned_in_qmd"] = list(qmd_nodes - sacred_nodes)
        report["missing_in_qmd"] = list(sacred_nodes - qmd_nodes)
        report["synced_correctly"] = list(qmd_nodes & sacred_nodes)
        
        # 4. åŸ·è¡Œæ¸…ç†ï¼ˆå¦‚æœä¸æ˜¯ dry_runï¼‰
        if not dry_run:
            # æ¸…ç†å­¤å…’è³‡æ–™ï¼šé‡æ–°åŒæ­¥æœ‰æ•ˆç¯€é»
            if report["missing_in_qmd"]:
                print(f"ğŸ”„ ç™¼ç¾ {len(report['missing_in_qmd'])} å€‹ç¯€é»éœ€è¦åŒæ­¥åˆ° QMD")
                # é€™è£¡å¯ä»¥é¸æ“‡è‡ªå‹•é‡æ–°åŒæ­¥
                # self.sync_from_sacred_essence(filter_states=None)
                report["actions_taken"].append("triggered_resync")
            
            if report["orphaned_in_qmd"]:
                print(f"ğŸ—‘ï¸  ç™¼ç¾ {len(report['orphaned_in_qmd'])} å€‹å­¤å…’è³‡æ–™åœ¨ QMD ä¸­")
                # QMD ç›®å‰æ²’æœ‰å–®å€‹åˆªé™¤ APIï¼Œéœ€è¦é‡å»ºç´¢å¼•
                # é€™è£¡æ¨™è¨˜ç‚ºéœ€è¦é‡å»º
                report["actions_taken"].append("needs_rebuild")
        
        return report
    
    # ==================== åŸæœ‰åŠŸèƒ½ï¼ˆå„ªåŒ–ç‰ˆï¼‰ ====================
    
    def sync_node_to_qmd(
        self, 
        node_id: str,
        topic: str,
        content: str,
        state: str = "SILVER",
        parent_id: Optional[str] = None
    ) -> bool:
        """å–®ç¯€é»åŒæ­¥ï¼ˆå¸¶è¦†å¯«ä¿è­·ï¼‰"""
        # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨ä¸”å…§å®¹ç›¸åŒ
        if self._is_node_synced(node_id, content):
            return True
        
        context_text = f"[NODE_ID:{node_id}][TOPIC:{topic}][STATE:{state}]"
        if parent_id:
            context_text += f"[PARENT:{parent_id}]"
        
        full_content = f"{context_text}\n{content}"
        
        temp_dir = Path.home() / ".cache" / "sacred-essence" / "qmd-sync"
        temp_dir.mkdir(parents=True, exist_ok=True)
        
        # ä½¿ç”¨å…§å®¹å“ˆå¸Œå‘½åï¼Œé¿å…é‡è¤‡
        import hashlib
        content_hash = hashlib.md5(full_content.encode()).hexdigest()[:8]
        temp_file = temp_dir / f"{topic}_{node_id}_{content_hash}.md"
        
        with open(temp_file, 'w', encoding='utf-8') as f:
            f.write(full_content)
        
        # æ¸…ç†èˆŠç‰ˆæœ¬
        for old_file in temp_dir.glob(f"{topic}_{node_id}_*.md"):
            if old_file.name != temp_file.name:
                old_file.unlink()
        
        if not self.collection_exists():
            success, _ = self._run_qmd([
                "collection", "add", str(temp_dir),
                "--name", self.collection_name,
                "--mask", "*.md"
            ])
        else:
            success, _ = self._run_qmd(["update"])
        
        if success:
            self._run_qmd(["embed", "-f"])
        
        return success
    
    def delete_node(self, node_id: str) -> bool:
        """å¾ QMD ä¸­åˆªé™¤ç‰¹å®šç¯€é» (é˜²ç¦¦ã€è³‡æ–™å¹½éˆã€)"""
        # QMD CLI åˆªé™¤æŒ‡ä»¤ (å‡è¨­æ”¯æ´ qmd delete <node_id> æˆ–é€éç§»é™¤åŸæª”å¾Œ update)
        # ç”±æ–¼åŸå§‹ç¢¼ä½¿ç”¨ `--mask *.md` è¼‰å…¥ temp_dirï¼Œåˆªé™¤ç­–ç•¥ç‚ºï¼š
        # å°‡è©²æª”æ¡ˆå¾ collection ä¸­ç§»é™¤ä¸¦ update
        # è‹¥ qmd æ”¯æ´: qmd collection remove-doc -c <name> <doc_id>
        print(f"ğŸ—‘ï¸  æ­£åœ¨å¾ QMD ä¸­ç§»é™¤å¹½éˆç¯€é»: {node_id}")
        
        # å‚™ç”¨æ–¹æ¡ˆï¼šæ‰¾åˆ°å«æœ‰è©² node_id çš„ cache file ä¸¦åˆªé™¤ï¼Œç„¶å¾Œé‡æ–° update
        temp_dir = Path.home() / ".cache" / "sacred-essence" / "qmd-sync"
        deleted_files = 0
        if temp_dir.exists():
            for f in temp_dir.glob(f"*_{node_id}_*.md"):
                f.unlink()
                deleted_files += 1
                
        if deleted_files > 0 and self.collection_exists():
            # é‡æ–°æ›´æ–°ç´¢å¼•ï¼Œè¢«åˆªé™¤çš„æª”æ¡ˆå°±æœƒå¾ QMD æ¶ˆå¤±
            success, _ = self._run_qmd(["update"])
            if success:
                self._run_qmd(["embed", "-f"])
                return True
        return False
    
    def _is_node_synced(self, node_id: str, content: str) -> bool:
        """æª¢æŸ¥ç¯€é»æ˜¯å¦å·²åŒæ­¥ä¸”å…§å®¹æœªè®Š"""
        # ç°¡åŒ–æª¢æŸ¥ï¼šå¯ä»¥æŸ¥è©¢ QMDï¼Œé€™è£¡å…ˆè¿”å› False ç¢ºä¿åŒæ­¥
        return False
    
    def constrained_search(
        self,
        query_text: str,
        node_whitelist: Set[str],
        n_results: int = 5,
        search_type: str = "hybrid"
    ) -> List[Dict]:
        """é™ç¸®æœç´¢ï¼ˆå„ªåŒ–ç‰ˆï¼Œå¸¶è¶…æ™‚æ§åˆ¶ï¼‰"""
        if not node_whitelist:
            return []
        
        # ä¿®è£œ Edge Case 3ï¼šé™åˆ¶æœç´¢ç¯„åœå¤§å°ï¼Œé¿å…éå¤šç¯€é»å°è‡´å»¶é²
        if len(node_whitelist) > 50:
            print(f"âš ï¸  ç™½åå–®ç¯€é»éå¤š ({len(node_whitelist)})ï¼Œåªå–å‰ 50 å€‹")
            node_whitelist = set(list(node_whitelist)[:50])
        
        if search_type == "vector":
            raw_results = self.vector_search(query_text, n_results=n_results * 2)
        elif search_type == "keyword":
            raw_results = self.keyword_search(query_text, n_results=n_results * 2)
        else:
            raw_results = self.query(query_text, n_results=n_results * 2)
        
        filtered_results = []
        for r in raw_results:
            content = r.get('content', '')
            node_id = self._extract_node_id_from_content(content)
            if node_id and node_id in node_whitelist:
                r['content'] = self._clean_content(content)
                r['node_id'] = node_id
                filtered_results.append(r)
                
                if len(filtered_results) >= n_results:
                    break
        
        return filtered_results
    
    def query(self, query_text: str, n_results: int = 5, min_score: Optional[float] = None) -> List[Dict]:
        """æ··åˆæœç´¢ï¼ˆå¸¶è¶…æ™‚ï¼‰"""
        args = ["query", query_text, "-n", str(min(n_results * 2, 20)), "--json"]
        if min_score:
            args.extend(["--min-score", str(min_score)])
        args.extend(["-c", self.collection_name])
        
        success, output = self._run_qmd(args, timeout=self.QMD_TIMEOUT)
        
        if success:
            try:
                results = json.loads(output)
                return results if isinstance(results, list) else []
            except:
                return []
        return []
    
    def vector_search(self, query_text: str, n_results: int = 5) -> List[Dict]:
        """å‘é‡æœç´¢ï¼ˆå¸¶è¶…æ™‚ï¼‰"""
        args = ["vsearch", query_text, "-n", str(min(n_results * 2, 20)), "--json"]
        args.extend(["-c", self.collection_name])
        
        success, output = self._run_qmd(args, timeout=self.QMD_TIMEOUT)
        
        if success:
            try:
                results = json.loads(output)
                return results if isinstance(results, list) else []
            except:
                return []
        return []
    
    def keyword_search(self, query_text: str, n_results: int = 5) -> List[Dict]:
        """BM25 é—œéµå­—æœç´¢ï¼ˆé€ƒç”Ÿè‰™ç”¨ï¼‰"""
        args = ["search", query_text, "-n", str(min(n_results, 10)), "--json"]
        args.extend(["-c", self.collection_name])
        
        success, output = self._run_qmd(args, timeout=self.QMD_TIMEOUT)
        
        if success:
            try:
                results = json.loads(output)
                return results if isinstance(results, list) else []
            except:
                return []
        return []
    
    def status(self) -> Dict:
        """ç‹€æ…‹æª¢æŸ¥"""
        success, output = self._run_qmd(["status"], timeout=5)
        if success:
            return {"status": "ok", "details": output}
        return {"status": "error", "error": output}
    
    def sync_from_sacred_essence(
        self, 
        memory_dir: Optional[str] = None,
        force: bool = False,
        filter_states: Optional[List[str]] = None
    ) -> bool:
        """æ‰¹é‡åŒæ­¥"""
        memory_dir = memory_dir or self.memory_dir
        
        if not os.path.exists(memory_dir):
            print(f"âŒ è¨˜æ†¶ç›®éŒ„ä¸å­˜åœ¨: {memory_dir}")
            return False
        
        temp_dir = Path.home() / ".cache" / "sacred-essence" / "qmd-sync"
        temp_dir.mkdir(parents=True, exist_ok=True)
        
        if force:
            for f in temp_dir.glob("*.md"):
                f.unlink()
        
        topics_dir = Path(memory_dir)
        synced_count = 0
        
        for content_file in topics_dir.rglob("content.md"):
            rel_path = content_file.relative_to(topics_dir)
            parts = rel_path.parts
            if len(parts) >= 2:
                topic = parts[0]
                node_id = parts[1]
                
                meta_file = content_file.parent / "node.meta.json"
                state = "SILVER"
                if meta_file.exists():
                    try:
                        with open(meta_file, 'r', encoding='utf-8') as f:
                            meta = json.load(f)
                        state = meta.get('state', 'SILVER')
                    except:
                        pass
                
                if filter_states and state not in filter_states:
                    continue
                
                with open(content_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # ä½¿ç”¨å…§å®¹å“ˆå¸Œé¿å…é‡è¤‡
                import hashlib
                metadata_prefix = f"[NODE_ID:{node_id}][TOPIC:{topic}][STATE:{state}]\n"
                full_content = metadata_prefix + content
                content_hash = hashlib.md5(full_content.encode()).hexdigest()[:8]
                
                sync_file = temp_dir / f"{topic}_{node_id}_{content_hash}.md"
                
                # åªå¯«å…¥è®Šæ›´çš„æª”æ¡ˆ
                if not sync_file.exists():
                    with open(sync_file, 'w', encoding='utf-8') as f:
                        f.write(full_content)
                    synced_count += 1
        
        print(f"ğŸ“¦ åŒæ­¥ {synced_count} å€‹è®Šæ›´çš„ç¯€é»åˆ° QMD...")
        
        if self.collection_exists() and not force:
            success, _ = self._run_qmd(["update"])
        else:
            if self.collection_exists():
                self._run_qmd(["collection", "remove", self.collection_name])
            success, _ = self._run_qmd([
                "collection", "add", str(temp_dir),
                "--name", self.collection_name,
                "--mask", "*.md"
            ])
        
        if success:
            self._run_qmd(["embed", "-f"])
            print(f"âœ… åŒæ­¥å®Œæˆ")
            return True
        return False


# ä¾¿æ·å‡½æ•¸
def create_bridge(collection_name: str = "sacred-l2", memory_dir: Optional[str] = None) -> QMDBridge:
    return QMDBridge(collection_name, memory_dir)


def sync_sacred_essence_to_qmd(
    collection_name: str = "sacred-l2",
    memory_dir: Optional[str] = None,
    filter_states: Optional[List[str]] = None,
    force: bool = False
) -> bool:
    """
    ä¾¿æ·å‡½æ•¸ï¼šåŒæ­¥ç¥é«“æ•¸æ“šåˆ° QMD
    
    Args:
        collection_name: QMD é›†åˆåç¨±
        memory_dir: ç¥é«“è¨˜æ†¶ç›®éŒ„è·¯å¾‘
        filter_states: åªåŒæ­¥æŒ‡å®šç‹€æ…‹çš„ç¯€é» (e.g., ["GOLDEN", "SILVER"])
        force: å¼·åˆ¶é‡æ–°å»ºç«‹é›†åˆ
    
    Returns:
        åŒæ­¥æ˜¯å¦æˆåŠŸ
    """
    bridge = QMDBridge(collection_name, memory_dir)
    return bridge.sync_from_sacred_essence(
        memory_dir=memory_dir,
        force=force,
        filter_states=filter_states
    )


if __name__ == "__main__":
    print("ğŸ§ª QMD Bridge v3.0 - Edge Cases ä¿®è£œç‰ˆ")
    print("åŠŸèƒ½ï¼šé€ƒç”Ÿè‰™ Fallback + æ•¸æ“šå¯©è¨ˆ + æ™ºèƒ½è¼‰å…¥\n")
    
    bridge = create_bridge("sacred-l2")
    
    # æ¸¬è©¦å¯©è¨ˆ
    print("ğŸ“Š åŸ·è¡Œæ•¸æ“šä¸€è‡´æ€§å¯©è¨ˆ...")
    report = bridge.audit_and_cleanup(dry_run=True)
    print(f"  æ­£ç¢ºåŒæ­¥: {len(report['synced_correctly'])} å€‹")
    print(f"  QMD å­¤å…’: {len(report['orphaned_in_qmd'])} å€‹")
    print(f"  ç¼ºå¤±åŒæ­¥: {len(report['missing_in_qmd'])} å€‹\n")
    
    # æ¸¬è©¦æ™ºèƒ½æœç´¢
    if bridge.collection_exists():
        print("ğŸ” æ¸¬è©¦æ™ºèƒ½æœç´¢ï¼ˆå«é€ƒç”Ÿè‰™ï¼‰...")
        results, meta = bridge.smart_search_with_fallback(
            query_text="æ¸¬è©¦",
            node_whitelist=set(),  # ç©ºç™½åå–®è§¸ç™¼ Fallback
            sacred_confidence=0.1,  # ä½ä¿¡å¿ƒè§¸ç™¼ Fallback
            n_results=3
        )
        print(f"  ç­–ç•¥: {meta['strategy']}")
        print(f"  è§¸ç™¼é€ƒç”Ÿè‰™: {meta['fallback_triggered']}")
        print(f"  çµæœæ•¸: {len(results)}")
        for r in results[:2]:
            print(f"    - [{r.score:.3f}] {r.node_id} ({r.source})")